{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6491300f",
   "metadata": {},
   "source": [
    "# Perceptron Model & MLP (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc71f6b",
   "metadata": {},
   "source": [
    "- [x] Dataset generation\n",
    "- [x] Perceptron\n",
    "    - [x] Activiation functions\n",
    "    - [x] init\n",
    "    - [x] fit\n",
    "    - [x] predict\n",
    "- [x] Neural Network\n",
    "    - [x] init\n",
    "    - [x] fit\n",
    "    - [x] predict\n",
    "- [x] Global script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a8335",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Here we import the different libraries and modules to run the code.\n",
    "And we add an autoreload feature to retrieve the last version of the python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d579451",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85de1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform\n",
    "\n",
    "from perceptron import Perceptron \n",
    "from neuralnetwork import NeuralNetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc6d5b",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "\n",
    "Not sure about the most appropriate form of the data. Therefore there is 2 different output functions:\n",
    "- generate_data_df: create the data as a Dataframe\n",
    "- generate_data_array: split the data into a outputs 2D array and a target 1D array\n",
    "\n",
    "Even if in the code, the target was set either -1 or 1, I decided to put it between 0 and 1.\n",
    "\n",
    "Not sure also about the value of features :\n",
    "- float from -1 to 1\n",
    "- boolean -1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab3afb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_df(size, features):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        size: (int) number of samples -> m\n",
    "        features : (int) number of features -> n\n",
    "    Return:\n",
    "        (Dataframe) with m samples labeled, n features [x0, xn] and the output y \n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        # dataset\n",
    "        [[uniform(-1.0, 1.0) for _ in range(features + 1)] for _ in range(size)],\n",
    "        # labels\n",
    "        columns=[str(f\"x{i}\") for i in range(features)] + ['y'])\n",
    "    \n",
    "def generate_dataset_array(size, features):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        size: (int) number of samples -> m\n",
    "        features : (int) number of features -> n\n",
    "    Return:\n",
    "        \n",
    "    \"\"\"\n",
    "    # value between -1. and 1\n",
    "    features = np.asarray([np.asarray([uniform(-1., 1.) for _ in range(features)]) for _ in range(size)])\n",
    "    # value between 0. and 1.\n",
    "#     features = np.asarray([np.asarray([uniform(0., 1.) for _ in range(features)]) for _ in range(size)])\n",
    "    # value 0 or 1\n",
    "#     features = np.asarray([np.random.randint(2, size=features) for _ in range(size)])\n",
    "\n",
    "    # value between -1. and 1.\n",
    "#     targets = np.heaviside(np.asarray([uniform(-1., 1.) for _ in range(size)]), 0)\n",
    "    # value 0 or 1\n",
    "    targets = np.random.randint(2, size=size)\n",
    "    targets = targets.reshape(np.shape(targets)[0], 1)\n",
    "    \n",
    "    \n",
    "    features[features==0] = -1.\n",
    "\n",
    "\n",
    "#     targets[targets==0] = -1\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc346d8c",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "Here we are creating a perceptron to check if the gradient descent is working.\n",
    "Apparently it is working for the 4 activiation function because of the error decreasing.\n",
    "Need some adjustement on the learing rate to not overshoot sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e641a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELU       Initial target: 0 | Model ouput: 3.0531133177191805e-16\n",
      "SIGMOID    Initial target: 0 | Model ouput: 0.06157841525911837\n",
      "HEAVISIDE  Initial target: 0 | Model ouput: 0.0\n",
      "TANH       Initial target: 0 | Model ouput: 6.10622663543836e-16\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "x, y = generate_dataset_array(1, 10)\n",
    "# Testing on the first element\n",
    "test_x, test_y = x[0], y[0][0]\n",
    "\n",
    "activation_functions = ['relu', 'sigmoid', 'heaviside', 'tanh']\n",
    "for activation_function in activation_functions:\n",
    "    perceptron = Perceptron(10, activation_function)\n",
    "    for _ in range(20):\n",
    "        perceptron.fit(test_x, test_y, 0.3)\n",
    "    print(f\"{activation_function.upper().ljust(10)} Initial target: {test_y} | Model ouput: {perceptron.predict(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4c364",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Here is the main part, with the context of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# TRAINING_LOOP = 500_000\n",
    "TRAINING_LOOP = 10000\n",
    "DATASET_SIZE = 8\n",
    "DATASET_FEATURES = 6\n",
    "LEARNING_RATE = 1.2\n",
    "\n",
    "neural_network = NeuralNetwork(6, [100], 3)\n",
    "# neural_network = NeuralNetwork(6, [10], 1)\n",
    "X, y = generate_dataset_array(DATASET_SIZE, DATASET_FEATURES)\n",
    "\n",
    "for i in range(TRAINING_LOOP):\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Training loop: {i*100//TRAINING_LOOP}%\")\n",
    "    for inputs, target in zip(X, y):\n",
    "        neural_network.fit(inputs, target, LEARNING_RATE)\n",
    "\n",
    "predictions = neural_network.predict(np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0]))\n",
    "print(f\"Prediction: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbdf001",
   "metadata": {},
   "source": [
    "### New Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8427cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_dataset():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Return:\n",
    "        2 2-D numpy array with the features inside, one is for training and the other one for testing\n",
    "        2 2-D numpy array with the targets inside, one is for training and the other one for testing\n",
    "    \"\"\"\n",
    "    # Load the dataset as an object\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the features\n",
    "    data_X = np.asarray(iris['data'])\n",
    "    # Take the targets\n",
    "    data_Y = np.asarray(iris['target'])\n",
    "    # Transform the 1-D array as a 2-D array\n",
    "    data_Y = data_Y.reshape(np.shape(data_Y)[0], 1)\n",
    "    \n",
    "    # Transform the the [0/1/2] label to a [1,0,0]/[0,0,0]/[0,0,1]\n",
    "    enc = OneHotEncoder()\n",
    "    data_Y = enc.fit_transform(data_Y).toarray()\n",
    "    \n",
    "    # Scale the features between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    data_X = scaler.fit_transform(data_X)\n",
    "    \n",
    "    # Split the data between a training and a testing part\n",
    "    return train_test_split(data_X, data_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f3013396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop: 0%\n",
      "Training loop: 10%\n",
      "Training loop: 20%\n",
      "Training loop: 30%\n",
      "Training loop: 40%\n",
      "Training loop: 50%\n",
      "Training loop: 60%\n",
      "Training loop: 70%\n",
      "Training loop: 80%\n",
      "Training loop: 90%\n",
      "Training loop: 100%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "TRAINING_LOOP = 10000\n",
    "LEARNING_RATE = 0.8\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = generate_dataset()\n",
    "\n",
    "INPUT_SIZE = len(X_train[0])\n",
    "OUTPUT_SIZE = len(Y_train[0])\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "    INPUT_SIZE,\n",
    "    [20, 20], \n",
    "    OUTPUT_SIZE,\n",
    ")\n",
    "\n",
    "for i in range(TRAINING_LOOP):\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Training loop: {i*100//TRAINING_LOOP}%\")\n",
    "    for inputs, target in zip(X_train, Y_train):\n",
    "        neural_network.fit(inputs, target, LEARNING_RATE)\n",
    "print(f\"Training loop: 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "269da71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = {}\n",
    "\n",
    "for inputs, target in zip(X_test, Y_test):\n",
    "    prediction = neural_network.predict(inputs)\n",
    "\n",
    "    # Put to the closest int the outputs\n",
    "    prediction = np.round(prediction)\n",
    "\n",
    "    # Storing for the translator\n",
    "    prediction_string = np.array2string(prediction)\n",
    "    target_string = np.array2string(target)\n",
    "    if prediction_string not in translator:\n",
    "        translator[prediction_string] = [target_string]\n",
    "    else:\n",
    "        translator[prediction_string].append(target_string)        \n",
    "\n",
    "convertor = {}\n",
    "for type_prediction in translator:\n",
    "    # If only one kind of output\n",
    "    if len(list(set(translator[type_prediction]))) == 1:\n",
    "        convertor[type_prediction] = translator[type_prediction][0]\n",
    "    else:\n",
    "        occurences = {}\n",
    "        for label in list(set(translator[type_prediction])):\n",
    "            occurences[label] = translator[type_prediction].count(label)\n",
    "        convertor[type_prediction] = max(occurences, key=occurences.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "bb321f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 / 38\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "for inputs, target in zip(X_test, Y_test):\n",
    "    prediction = neural_network.predict(inputs)\n",
    "    prediction = np.round(prediction)\n",
    "    prediction_string = convertor[np.array2string(prediction)]\n",
    "    target_string = np.array2string(target)\n",
    "    if prediction_string == target_string:\n",
    "        correct_predictions += 1\n",
    "print(correct_predictions, '/', len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a436b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
