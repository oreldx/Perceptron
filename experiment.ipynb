{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6491300f",
   "metadata": {},
   "source": [
    "# Perceptron Model & MLP (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc71f6b",
   "metadata": {},
   "source": [
    "- [x] Dataset generation\n",
    "- [x] Perceptron\n",
    "    - [x] Activiation functions\n",
    "    - [x] init\n",
    "    - [x] fit\n",
    "    - [x] predict\n",
    "- [x] Neural Network\n",
    "    - [x] init\n",
    "    - [x] fit\n",
    "    - [x] predict\n",
    "- [x] Global script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a8335",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Here we import the different libraries and modules to run the code.\n",
    "And we add an autoreload feature to retrieve the last version of the python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d579451",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85de1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from random import uniform\n",
    "from pprint import pprint\n",
    "\n",
    "from perceptron import Perceptron \n",
    "from neuralnetwork import NeuralNetwork \n",
    "from neuralnetworkzouk import NeuralNetworkZouk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc6d5b",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "\n",
    "Not sure about the most appropriate form of the data. Therefore there is 2 different output functions:\n",
    "- generate_data_df: create the data as a Dataframe\n",
    "- generate_data_array: split the data into a outputs 2D array and a target 1D array\n",
    "\n",
    "Even if in the code, the target was set either -1 or 1, I decided to put it between 0 and 1.\n",
    "\n",
    "Not sure also about the value of features :\n",
    "- float from -1 to 1\n",
    "- boolean -1 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ab3afb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_df(size, features):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        size: (int) number of samples -> m\n",
    "        features : (int) number of features -> n\n",
    "    Return:\n",
    "        (Dataframe) with m samples labeled, n features [x0, xn] and the output y \n",
    "    \"\"\"\n",
    "    return pd.DataFrame(\n",
    "        # dataset\n",
    "        [[uniform(-1.0, 1.0) for _ in range(features + 1)] for _ in range(size)],\n",
    "        # labels\n",
    "        columns=[str(f\"x{i}\") for i in range(features)] + ['y'])\n",
    "    \n",
    "def generate_dataset_array(size, features):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        size: (int) number of samples -> m\n",
    "        features : (int) number of features -> n\n",
    "    Return:\n",
    "        \n",
    "    \"\"\"\n",
    "    # value between -1. and 1\n",
    "    features = np.asarray([np.asarray([uniform(-1., 1.) for _ in range(features)]) for _ in range(size)])\n",
    "    # value between 0. and 1.\n",
    "#     features = np.asarray([np.asarray([uniform(0., 1.) for _ in range(features)]) for _ in range(size)])\n",
    "    # value 0 or 1\n",
    "#     features = np.asarray([np.random.randint(2, size=features) for _ in range(size)])\n",
    "\n",
    "    # value between -1. and 1.\n",
    "#     targets = np.heaviside(np.asarray([uniform(-1., 1.) for _ in range(size)]), 0)\n",
    "    # value 0 or 1\n",
    "    targets = np.random.randint(2, size=size)\n",
    "    targets = targets.reshape(np.shape(targets)[0], 1)\n",
    "    \n",
    "    \n",
    "    features[features==0] = -1.\n",
    "\n",
    "\n",
    "#     targets[targets==0] = -1\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc346d8c",
   "metadata": {},
   "source": [
    "### Perceptron\n",
    "\n",
    "Here we are creating a perceptron to check if the gradient descent is working.\n",
    "Apparently it is working for the 4 activiation function because of the error decreasing.\n",
    "Need some adjustement on the learing rate to not overshoot sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0e641a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RELU       Initial target: 0 | Model ouput: 3.0531133177191805e-16\n",
      "SIGMOID    Initial target: 0 | Model ouput: 0.06157841525911837\n",
      "HEAVISIDE  Initial target: 0 | Model ouput: 0.0\n",
      "TANH       Initial target: 0 | Model ouput: 6.10622663543836e-16\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "x, y = generate_dataset_array(1, 10)\n",
    "# Testing on the first element\n",
    "test_x, test_y = x[0], y[0][0]\n",
    "\n",
    "activation_functions = ['relu', 'sigmoid', 'heaviside', 'tanh']\n",
    "for activation_function in activation_functions:\n",
    "    perceptron = Perceptron(10, activation_function)\n",
    "    for _ in range(20):\n",
    "        perceptron.fit(test_x, test_y, 0.3)\n",
    "    print(f\"{activation_function.upper().ljust(10)} Initial target: {test_y} | Model ouput: {perceptron.predict(test_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4c364",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "\n",
    "Here is the main part, with the context of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbcf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload\n",
    "\n",
    "# TRAINING_LOOP = 500_000\n",
    "TRAINING_LOOP = 10000\n",
    "DATASET_SIZE = 8\n",
    "DATASET_FEATURES = 6\n",
    "LEARNING_RATE = 1.2\n",
    "\n",
    "neural_network = NeuralNetwork(6, [100], 3)\n",
    "# neural_network = NeuralNetwork(6, [10], 1)\n",
    "X, y = generate_dataset_array(DATASET_SIZE, DATASET_FEATURES)\n",
    "\n",
    "for i in range(TRAINING_LOOP):\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Training loop: {i*100//TRAINING_LOOP}%\")\n",
    "    for inputs, target in zip(X, y):\n",
    "        neural_network.fit(inputs, target, LEARNING_RATE)\n",
    "\n",
    "predictions = neural_network.predict(np.array([-1.0, -1.0, -1.0, -1.0, -1.0, -1.0]))\n",
    "print(f\"Prediction: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23f7e99",
   "metadata": {},
   "source": [
    "### New Dataset - Iris\n",
    "\n",
    "This data sets consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "\n",
    "The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.\n",
    "\n",
    "![dataset_presentation](https://deeplearning.cms.waikato.ac.nz/img/iris.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310d020",
   "metadata": {},
   "source": [
    "Retrieving the dataset and formatting it in the correct ouput for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4f9ea79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_dataset():\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Return:\n",
    "        2 2-D numpy array with the features inside, one is for training and the other one for testing\n",
    "        2 2-D numpy array with the targets inside, one is for training and the other one for testing\n",
    "    \"\"\"\n",
    "    # Load the dataset as an object\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the features\n",
    "    data_X = np.asarray(iris['data'])\n",
    "    # Take the targets\n",
    "    data_Y = np.asarray(iris['target'])\n",
    "    # Transform the 1-D array as a 2-D array\n",
    "    data_Y = data_Y.reshape(np.shape(data_Y)[0], 1)\n",
    "    \n",
    "    # Transform the the [0/1/2] label to a [1,0,0]/[0,0,0]/[0,0,1]\n",
    "    enc = OneHotEncoder()\n",
    "    data_Y = enc.fit_transform(data_Y).toarray()\n",
    "    \n",
    "    # Scale the features between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    data_X = scaler.fit_transform(data_X)\n",
    "    \n",
    "    # Split the data between a training and a testing part\n",
    "    return train_test_split(data_X, data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40aca86",
   "metadata": {},
   "source": [
    "Training the model, here a 3 hidden layers with 20 perceptrons each (2 layers of 20 had perform the same), followed by a 3 outputs layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43ae5d08",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15797/4140983772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mINPUT_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "TRAINING_LOOP = 10000\n",
    "LEARNING_RATE = 0.8\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = generate_dataset()\n",
    "\n",
    "INPUT_SIZE = len(X_train[0])\n",
    "OUTPUT_SIZE = len(Y_train[0])\n",
    "\n",
    "neural_network = NeuralNetworkZouk(\n",
    "    INPUT_SIZE,\n",
    "    [(20, 'relu'), (20, 'relu')], \n",
    "    (OUTPUT_SIZE, 'tanh'),\n",
    ")\n",
    "\n",
    "for i in range(TRAINING_LOOP):\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Training loop: {i*100//TRAINING_LOOP}%\")\n",
    "    for inputs, target in zip(X_train, Y_train):\n",
    "        neural_network.fit(inputs, target, LEARNING_RATE)\n",
    "print(f\"Training loop: 100%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd077e17",
   "metadata": {},
   "source": [
    "Because the output format of the neural network is not understandable directy, I built a translator between the label and the direct ouput.\n",
    "\n",
    "For that we have created a dictionary with 3 tables having for key the different labels of the dataset.\n",
    "We then make predictions and place them in the associated table.\n",
    "We match an output type by label. A label can have several output types.\n",
    "But if ever several labels correspond to an output type, we select the label that has the most occurrences for this output type.\n",
    "\n",
    "2 limitations of this system:\n",
    "- If an output gives in quasi equality different labels, we lose 50% of the predictions (well after it is always better to have a chance on three than a chance on two)\n",
    "- If the labeled data have different features than the real data, the coherence in the prediction will be questioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "8c839b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookup_table(\n",
    "    features,\n",
    "    targets,\n",
    "    model,\n",
    "):\n",
    "    translator = {}\n",
    "\n",
    "    for inputs, target in zip(features, targets):\n",
    "        prediction = model.predict(inputs)\n",
    "\n",
    "        # Put to the closest int the outputs\n",
    "        prediction = np.round(prediction)\n",
    "\n",
    "        # Storing for the translator\n",
    "        prediction_string = np.array2string(prediction)\n",
    "        target_string = np.array2string(target)\n",
    "        if prediction_string not in translator:\n",
    "            translator[prediction_string] = [target_string]\n",
    "        else:\n",
    "            translator[prediction_string].append(target_string)        \n",
    "\n",
    "    convertor = {}\n",
    "    for type_prediction in translator:\n",
    "        # If only one kind of output\n",
    "        if len(list(set(translator[type_prediction]))) == 1:\n",
    "            convertor[type_prediction] = translator[type_prediction][0]\n",
    "        else:\n",
    "            occurences = {}\n",
    "            for label in list(set(translator[type_prediction])):\n",
    "                occurences[label] = translator[type_prediction].count(label)\n",
    "            print(\"Multi labels same output: \", occurences)\n",
    "            convertor[type_prediction] = max(occurences, key=occurences.get)\n",
    "    \n",
    "    return convertor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "88d7cbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi labels same output:  {'[0.]': 4, '[0.5]': 36, '[1.]': 32}\n",
      "Prediction rate 52% | 20/38\n"
     ]
    }
   ],
   "source": [
    "lookupt_table = create_lookup_table(X_train, Y_train, neural_network)\n",
    "\n",
    "correct_predictions = 0\n",
    "for inputs, target in zip(X_test, Y_test):\n",
    "    prediction = neural_network.predict(inputs)\n",
    "    prediction = np.round(prediction)\n",
    "    if np.array2string(prediction) in lookupt_table:\n",
    "        prediction_string = lookupt_table[np.array2string(prediction)]\n",
    "        target_string = np.array2string(target)\n",
    "        if prediction_string == target_string:\n",
    "            correct_predictions += 1\n",
    "print(f\"Prediction rate {correct_predictions*100//len(Y_test)}% | {correct_predictions}/{len(Y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d2e24e",
   "metadata": {},
   "source": [
    "### New model - Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08c00771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loop: 0%\n",
      "Training loop: 10%\n",
      "Training loop: 20%\n",
      "Training loop: 30%\n",
      "Training loop: 40%\n",
      "Training loop: 50%\n",
      "Training loop: 60%\n",
      "Training loop: 70%\n",
      "Training loop: 80%\n",
      "Training loop: 90%\n",
      "Training loop: 100%\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_dataset():\n",
    "    # Load the dataset as an object\n",
    "    iris = datasets.load_iris()\n",
    "    # Take the features\n",
    "    data_X = np.asarray(iris['data'])\n",
    "    # Take the targets\n",
    "    data_Y = np.asarray(iris['target'])\n",
    "    \n",
    "    # Scale between 0 and 1\n",
    "    scaler = MinMaxScaler()\n",
    "    data_X = scaler.fit_transform(data_X)\n",
    "    data_Y = data_Y.reshape(np.shape(data_Y)[0], 1)\n",
    "    data_Y = scaler.fit_transform(data_Y)\n",
    "    \n",
    "    # Split the data between a training and a testing part\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(data_X, data_Y)    \n",
    "    return X_train, X_test, Y_train, Y_test, scaler\n",
    "\n",
    "\n",
    "TRAINING_LOOP = 10000\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "X_train, X_test, Y_train, Y_test, scaler = generate_dataset()\n",
    "\n",
    "INPUT_SIZE = len(X_train[0])\n",
    "OUTPUT_SIZE = 3\n",
    "\n",
    "neural_network = NeuralNetwork(\n",
    "    INPUT_SIZE,\n",
    "    [(20, 'sigmoid'), (20, 'sigmoid')],\n",
    "    (OUTPUT_SIZE, 'sigmoid'),\n",
    ")\n",
    "\n",
    "for i in range(TRAINING_LOOP):\n",
    "    if (i%1000 == 0):\n",
    "        print(f\"Training loop: {i*100//TRAINING_LOOP}%\")\n",
    "    for inputs, target in zip(X_train, Y_train):\n",
    "        neural_network.fit(inputs, target[0], LEARNING_RATE)\n",
    "print(f\"Training loop: 100%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2af666d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_look_up(features, targets, model):\n",
    "    preds = [model.predict(inputs) for inputs in features]\n",
    "    stats = {}\n",
    "    for pred, target in zip(preds, targets):\n",
    "        target_string = np.array2string(target)\n",
    "        if target_string not in stats:\n",
    "            stats[target_string] = [pred]\n",
    "        else:\n",
    "            stats[target_string].append(pred)\n",
    "    for label in stats:\n",
    "        values = np.asarray(stats[label])\n",
    "        print(label, np.mean(values, axis=0), np.mean(values))\n",
    "        print(label, np.min(values, axis=0), '\\n', np.max(values, axis=0), '\\n')\n",
    "    pprint(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c8ab2bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label [0]: Succes rate 100.0%\n",
      "Label [1]: Succes rate 100.0%\n",
      "Label [2]: Succes rate 75.0%, missclassification of 4 items over 16\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "predictions = [neural_network.predict(inputs) for inputs in X_test]\n",
    "\n",
    "stats = {}\n",
    "for pred, target in zip(predictions, scaler.inverse_transform(Y_test)):\n",
    "    idx = np.argmax(softmax(pred))\n",
    "    if idx not in stats:\n",
    "        stats[idx] = [int(target[0])]\n",
    "    else:\n",
    "        stats[idx].append(int(target[0]))\n",
    "  \n",
    "occurences = {}\n",
    "for idx in range(3):\n",
    "    occurences[idx] = {}\n",
    "    for label in list(set(stats[idx])):\n",
    "        occurences[idx][label] = stats[idx].count(label)\n",
    "\n",
    "for key in occurences:\n",
    "    total = 0\n",
    "    for label in occurences[key]:\n",
    "        total += occurences[key][label]\n",
    "    output = f\"Label [{key}]: Succes rate {occurences[key][label]*100/total}%\"\n",
    "    if occurences[key][label] != total:\n",
    "        output += f\", missclassification of {total-occurences[key][label]} items over {total}\"\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0fede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
